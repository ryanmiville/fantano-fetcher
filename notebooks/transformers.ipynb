{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import pipeline, ViTForImageClassification, ViTImageProcessor, ViTFeatureExtractor, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitedImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, limit_per_class=150):\n",
    "        super(LimitedImageFolder, self).__init__(root, transform, target_transform)\n",
    "        self.limit_per_class = limit_per_class\n",
    "        self._limit_dataset()\n",
    "\n",
    "    def _limit_dataset(self):\n",
    "        class_counts = dict()\n",
    "        new_samples = []\n",
    "        for sample, target in self.samples:\n",
    "            if target not in class_counts:\n",
    "                class_counts[target] = 0\n",
    "            if class_counts[target] < self.limit_per_class:\n",
    "                new_samples.append((sample, target))\n",
    "                class_counts[target] += 1\n",
    "        self.samples = new_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data\"\n",
    "\n",
    "# Set the batch size and number of training epochs\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "# Load the image data\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to the desired input size of the model\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "])\n",
    "\n",
    "dataset = LimitedImageFolder(data_root, transform=image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "for i, class_name in enumerate(dataset.classes):\n",
    "    label2id[class_name] = str(i)\n",
    "    id2label[str(i)] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationCollator:\n",
    "    def __init__(self, feature_extractor):\n",
    "        self.feature_extractor = feature_extractor\n",
    " \n",
    "    def __call__(self, batch):\n",
    "        encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')\n",
    "        encodings['labels'] = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
    "        return encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ryanmiville/dev/fantano-fetcher/ml/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/ryanmiville/dev/fantano-fetcher/ml/.venv/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2c57db23f64f50b05611d052ee0266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1498.9521, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.1, 'train_loss': 0.7241793823242187, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained ViT model and its feature extractor\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: other\n",
      "Predicted Score: 0.6961593627929688\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the saved fine-tuned model\n",
    "model_path = \"./fine_tuned_model\"\n",
    "\n",
    "# Load the fine-tuned model and feature extractor\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Define the image classification pipeline\n",
    "image_classifier = pipeline(\"image-classification\", feature_extractor=feature_extractor, image_processor=image_processor, model=model)\n",
    "\n",
    "def perform_inference(image_path: str):\n",
    "    results = image_classifier(image_path)\n",
    "    # Get the predicted label and its corresponding class name\n",
    "    predicted_label = results[0]['label']\n",
    "    predicted_score = results[0]['score']\n",
    "    return (predicted_label, predicted_score)\n",
    "\n",
    "# Set the path to the new image for inference\n",
    "yellow_example = \"./data/yellow/zJ5A5CdlnTo.jpg\"\n",
    "other_example = \"./data/other/14sLvv_ykmE.jpg\"\n",
    "# Perform inference\n",
    "(label, score) = perform_inference(yellow_example)\n",
    "# Print the predicted label and score\n",
    "print(\"Predicted Label:\", label)\n",
    "print(\"Predicted Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('other', 0.6961593627929688)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_inference(yellow_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
